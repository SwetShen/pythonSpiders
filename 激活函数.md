## 什么是激活函数

激活函数是深度学习，亦或者说人工神经网络中一个十分重要的组成部分，它可以对神经元的接收信息进行非线性变换，将变换后的信息输出到下一层神经元。激活函数作用方式如下公式所示：

<img src="file:///C:/Users/archerswet/AppData/Roaming/marktext/images/2022-10-25-19-41-46-image.png" title="" alt="" width="314">

其中，就是激活函数。

为什么要使用激活函数呢？当我们不用激活函数时，网络中各层只会根据权重和偏差只会进行线性变换，就算有多层网络，也只是相当于多个线性方程的组合，依然只是相当于一个线性回归模型，解决复杂问题的能力有限。我们希望我们的神经网络能够处理复杂任务，如语言翻译和图像分类等，线性变换永远无法执行这样的任务。激活函数得加入能对输入进行非线性变换，使其能够学习和执行更复杂的任务。

另外，激活函数使反向传播成为可能，因为激活函数的误差梯度可以用来调整权重和偏差。如果没有可微的非线性函数，这就不可能实现。

总之，激活函数的作用是能够给神经网络加入一些非线性因素，使得神经网络可以更好地解决较为复杂的问题。

## 常用激活函数

### sigmod

sigmoid函数可以将整个实数范围的的任意值映射到[0,1]范围内，当当输入值较大时,sigmoid将返回一个接近于1的值,而当输入值较小时,返回值将接近于0。

<img title="" src="https://mmbiz.qpic.cn/mmbiz_png/njjfaJS7c9rCMpP2obr45mrr5eGhibL1RAq4B8KcteHZwhkpB54icD95H0sicUeZvRSzSbqwccwecuA9cUJnXpGqA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="">

```
import tensorflow as tf
x = tf.linspace(-5.,5.,6)
x
```

两种方式的sigmod函数调用方式：

```
tf.keras.activations.sigmoid(x)
```

```
tf.sigmoid(x)
```

### relu

Relu（Rectified Linear Units修正线性单元），是目前被使用最为频繁得激活函数，relu函数在x<0时，输出始终为0。由于x>0时，relu函数的导数为1，即保持输出为x，所以relu函数能够在x>0时保持梯度不断衰减，从而缓解梯度消失的问题，还能加快收敛速度，还能是神经网络具有稀疏性表达能力，这也是relu激活函数能够被使用在深层神经网络中的原因。由于当x<0时，relu函数的导数为0，导致对应的权重无法更新，这样的神经元被称为"神经元死亡"。

<img title="" src="https://mmbiz.qpic.cn/mmbiz_png/njjfaJS7c9rCMpP2obr45mrr5eGhibL1RIVBdrdGfMXTnyObDXibFu46IJicC5UU28Ol73DE6TSL5icT0h7iaDL8NvQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="">

tf.keras.activations.relu( x, alpha=0.0, max_value=None, threshold=0 )

- x：输入的变量
- alpha：上图中左半边部分图像的斜率，也就是x值为负数（准确说应该是小于threshold）部分的斜率，默认为0
- max_value：最大值，当x大于max_value时，输出值为max_value
- threshold：起始点，也就是上面图中拐点处x轴的值

```
import tensorflow as tf
x = tf.linspace(-5.,5.,6)
tf.keras.activations.relu(x)
```

```
tf.keras.activations.relu(x,alpha=2.)
```

```
tf.keras.activations.relu(x,max_value=2.)  
# 大于2部分都将输出为2.
```

```
tf.keras.activations.relu(x,alpha=2., threshold=3.5) 
 # 小于3.5的值按照alpha * (x - threshold)计算
```

### softmax

softmax函数是sigmoid函数的进化，在处理分类问题是很方便，它可以将所有输出映射到成概率的形式，即值在[0,1]范围且总和为1。例如输出变量为[1.5,4.4,2.0]，经过softmax函数激活后，输出为[0.04802413, 0.87279755, 0.0791784 ],分别对应属于1、2、3类的概率。

```
tf.nn.softmax(tf.constant([[1.5,4.4,2.0]]))
```

```
tf.keras.activations.softmax(tf.constant([[1.5,4.4,2.0]]))
```

```
x = tf.random.uniform([1,5],minval=-2,maxval=2)
tf.keras.activations.softmax(x)
```

### tanh

tanh函数无论是功能还是函数图像上sigmoid函数十分相似，所以两者的优缺点也一样，区别在于tanh函数将值映射到[-1,1]范围

<img title="" src="https://mmbiz.qpic.cn/mmbiz_png/njjfaJS7c9rCMpP2obr45mrr5eGhibL1RLseDYolCJfkh5akNMke2ejIndNlnAEXK5sUIcBHTbw1PWMDZm08GoA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="">

```
x = tf.linspace(-5., 5.,6)
tf.keras.activations.tanh(x)
```
